% Chapter Template

\chapter{Theory} % Main chapter title
This chapter focuses on explaining the theory behind the model and the methods used in this work.
Details regarding the exact problem definition, model adjustments, programs used and so on are given in
\hyperref[chap:methodology]{Chapter \ref*{chap:methodology} - Methodology}.

\label{chap:theory} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}


%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------
\section{Background of computational disease modeling}
A common goal for disease modeling is to predict the spreading of a disease in a given population. Different approaches can be made to
achieve this goal. In 2019 Akhtar et al. presented a model, based on a neural network, that tries to predict the spread of the Zika virus\cite{note}.
Models like this are promising, but have the disadvantage of requiring big sets of data in order to train the neural network in the first place.
(\textcolor{red}{check article if this is actually the case!}) % note
Other models are probabilistic in nature and try predict the spreading of a disease by simulating an individuals chance of contracting a
disease at a given point in time\cite{??}. These models rely on big population counts to compensate errors that are caused by the random nature.
In addition modeling the infection chances of each individual in a given population often requires considerable computational resources\cite{??}.
A third method of computational disease modeling is based on differential equations. This type of model tries to establish and express relations between different
properties of a system. In disease modeling this could mean an increase in infection rate based on the population density of a modeled region or a
change in population mobility based on the average number of cars owned per household\cite{??}. The challenging aspect of this method is the design and
optimization of the model itself and its equations, since real life phenomena (i.e. an increase in population mobility) tend to be influenced by many
different other factors (i.e. a high density of personal transportation, holiday seasons, cultural backgrounds). This means that the relations between many
different ``moving parts'' in a system need to be identified, related to one another and then modeled and tuned correctly, in order to make accurate predictions
of future events. However, if this modeling attempt is successful, it not only serves as a solid base for predictions of future events, it also provides insight
into the interconnected nature of a system.


%----------------------------------------------------------------------------------------
%	SECTION 2
%----------------------------------------------------------------------------------------
\section{Compartmental modeling techniques}
Compartmental modelings revere to a modeling technique often used to systematically describe real world phenomena
like disease spreading via differential equations\cite{compartementMod}. In these models the population is divided into different groups. 
Members of one group can move to another group. This movement is based on said differential equations, that are used to model the real
world behavior of each group. Notable examples for these types of techniques are the ``SIR'' and the ``SEIRD'' model, which are discussed in
the following sections. The latter of the two models was used in this work.


%-----------------------------------
%	SUBSECTION 1
%-----------------------------------
\subsection{Differential equations}
\textcolor{red}{add citations}\\ %note
%\textcolor{red}{kinda shit, add more information, starting condition, etc.}\\ %note
In order to understand the previously mentioned models, we will first introduce the concept of differential equations.
Differential equations are functions which contain derivatives of unknown functions, which in turn depend
on either one or more independent variables. Depending on the number of independent variables the differential
equations are either considered \I{Ordinary Differential Equations} (ODE) (in case of a single variable, often time) or
\I{Partial Differential Equations} (PDE) (in case of multiple variables). An example for each type of equation
is given below, where equation \ref*{eq:ODE_ex} shows a simple \I{ODE} and equation \ref*{eq:PDE_ex} shows a
simple \I{PDE}.\newline

\begin{align}
	\frac{\delta u}{\delta x} + b = c \label{eq:ODE_ex}\\
	\frac{\delta u}{\delta x} + \frac{\delta u}{\delta y} \cdot \frac{\delta^2 u}{\delta z^2} = c \label{eq:PDE_ex}
\end{align}

Differential equations are often used to describe the rate of change in a given quantity in relation to another
changing quantity. This makes them very useful for describing complex biological, physical, mechanical, economical
or other scientific systems, where multiple system parameters interact with one another. However, while differential
equations are a powerful tool in describing dynamic systems, it is often difficult to find their solution (if such
a solution even exists). Solving a differential equation describes the act of finding a definition of the
unknown functions, which satisfies both the differential equation and possible other conditions, such as starting
conditions.


%-----------------------------------
%	SUBSECTION 2
%-----------------------------------
\subsection{The SIR model}
\label{sec:SIR}
In 1927 Kermack and McKendrick\cite{kermack1991contributions} first introduced their method of mathematically describing  
the epidemiological behavior of an infectious disease. The model divides a population into three distinct groups.

\begin{enumerate}[label=$\bullet$]
	\item \B{Susceptibles (S)}: Number of Individuals that are not immune to the infection.
		%If in contact with the virus these individuals can migrate to the\linebreak ``Infected'' group
	\item \B{Infected (I)}: Number of Individuals that are currently infected with the disease.
		%Infected individuals contribute to the infection of members of the susceptible group.
		%At some point during their infection, these members transition to the ``Removed'' group.
	\item \B{Removed (R)}: Number of Individuals that  have either overcome an infection and are now immune or have succumb to the disease.
		%and are diseased. They do not spread the virus and cannot be infected again.
\end{enumerate}


Population changes in all three groups are described in the form of ordinary differential equations\cite{mathSIR}.
Members of \B{S} can transition to \B{I}. This transition rate is expressed in the
form of variable $\alpha$, which is a positive, real number. This parameter includes many different epidemcal factors like
population density, infectiousness and many more and tries to condense them into one single value.
Transition also occurs from \B{I} to \B{R}. This change is expressed by the variable 
$\beta$, which is a positive constant between 0 and 1. $\beta$ can be rewritten as $\frac{1}{b}$, where $b$ is the average
duration an infected individual remains contagious, before it either recovers or dies. The concept of the SIR model is
illustrated in \hyperref[fig:SIR]{Figure \ref*{fig:SIR}}. The mathematical expression of the equations are shown below, where
$t$ represents a point in time.

\begin{figure}
	\begin{center}
		\includegraphics[width=0.75\textwidth]{./figures/SIR.png}
		\caption{Illustration of the population flow in the SIR model. S represents the ``Susceptible'', I
			the ``Infected'' and R the ``Removed'' population. The variable $\beta$ can also redefined
			as $\frac{1}{b}$, where $D$ is the average duration of contagiousness}
		\label{fig:SIR}
	\end{center}
\end{figure}


\begin{align}
	\label{eq:SIR1}
	\frac{dS}{dt} &= -\alpha S I \\
	\frac{dI}{dt} &= \alpha S I - \beta I \\
	\frac{dR}{dt} &= \beta I
\end{align}


The model also assumes, that the total number of individuals in the system $N$ remains constant in every time step and
that the sum of all transitions between all three groups during every time step remains zero. This is expressed in the equations below.
$S(t)$, $I(t)$ and $R(t)$ express the number of individuals in each of the groups, at time $t$, respectively.

\begin{align}
	\label{eq:SIR2}
	I(t) + S(t) + R(t) &= N \\
	\frac{dS}{dt} + \frac{dI}{dt} + \frac{dR}{dt} &= 0
\end{align}


Since the model assumes, that the total population remains stable, phenomena like immigration emigration or child birth are not accounted
for. This does not correctly represent the real world. However, in this model it is assumed, that population fluctuations
due to these occurrences are minor enough compared to the entirety of a regions population, that they do not alter the results of the
simulation in a major way\cite{??}.\newline


In order to correctly model the dynamics of an epidemic, the variables of these differential
equations (like $\alpha$ and $\beta$ in this case) must be determined. This is part of solving the differential equations.
Different methods can be applied to achieve this goal. Two of these techniques are the \hyperref[sec:Gauss]{Gauss-Newton algorithm}\cite{Gauss??}
and \hyperref[sec:PSO]{Particle Swarm Optimization (PSO)}\cite{PSO??}. Both of these techniques were used in this work and will are explained
later in this chapter.


%-----------------------------------
%	SUBSECTION 3
%-----------------------------------
\subsection{The SEIRD model}
\label{sec:SEIRD}
The SEIRD model is a variant of the simpler SIR model\cite{knodel20173d} and aims to improve upon its predecessor, by
introducing two new groups of individuals. These groups are used to differentiate the mechanics of an epidemic much
more precise.
\textcolor{red}{(double check citation)}. % note
The five groups of the SEIRD model are shown below.

\begin{enumerate}[label=$\bullet$]
	\item \B{Susceptibles (S)}: Individuals that are not immune to the infection.
	\item \B{Exposed (E)}: Individuals that are infected, but show no or very little symptoms. These individuals
		do not quarantine yet and are therefor contributing to the spread of the disease.
	\item \B{Infected (I)}: Individuals that are severely sick and therefor hospitalized.
	\item \B{Recovered (R)}: Individuals that  have overcome the infection and are now immune to the disease.
	\item \B{Diseased (D)}: Individuals that have succumb to the infection.
\end{enumerate}


As described in the previous section, the transition between the individuals of different groups is the core
part of the model. The modified equations are shown below.

\begin{align}
	%\label{eq:SEIRD1}
	\frac{dS}{dt} &= -\alpha S E \label{eq:SEIRD1_S} \\
	\frac{dE}{dt} &= \alpha S E -\frac{1}{q} E \label{eq:SEIRD1_E} \\
	\frac{dI}{dt} &= \frac{\kappa}{q} E - \frac{1}{p} I \label{eq:SEIRD1_I} \\
	\frac{dR}{dt} &= \frac{1-\kappa}{q} E + \frac{1-\tau}{p} I \label{eq:SEIRD1_R} \\
	\frac{dD}{dt} &= \frac{\tau}{p} I \label{eq:SEIRD1_D} 
\end{align}

The transition between the \B{S} and \B{E} population in SEIRD is equivalent to the transition
between \B{S} and \B{I} in SIR. However, the population outflow from \B{E} is split and can either move
to \B{I} or \B{R}. This allows the modeling of infections that end in a quick recovery or an infection
with complications (defined by the hospitalization of the patient). The ratio between the two scenarios is governed by the variable
$\kappa$. In addition, the variable $q$ was introduced and now represents the average duration until recovery/hospitalization of an
infected individual occurs. Similarly, the outflow of \B{I} is split between the populations of \B{R} and \B{D}. The ratio is governed
by the variable $\tau$. The equations for \B{R} and \B{D} were also adjusted to represent this change. Lastly the variable $p$ was
introduced, which now represents the average stay of a hospitalized individual until they either recover or succumb to the infection.
\hyperref[fig:SEIRD]{Figure \ref*{fig:SEIRD}} illustrates all these changes.\\

\begin{figure}
	\begin{center}
		\includegraphics[width=0.75\textwidth]{./figures/SEIRD.png}
		\caption{Illustration of the population flow in the SEIRD model. S represents the ``Susceptible'', E the ``Exposed'',
			I the ``Infected'', R the ``Recovered'' and D the ``Diseased'' population. $\beta$ can also be expressed as
			$\frac{1}{\phi}$, where $\phi$ is the average time to recovery/hospitalization.}
		\label{fig:SEIRD}
	\end{center}
\end{figure}



Like SIR, the SEIRD model assumes that the total number of individuals $N$ in the population remains constant during every step
of the modeling process. This leads to the equations expressed below. Again, $S(t)$, $E(t)$, $I(t)$, $R(t)$ and $D(t)$ are
expressing the number of individuals for each group at time $t$, respectively. $N$ remains the total number of individuals in the model.

\begin{align}
	\label{eq:SEIRD2}
	S(t) + E(t) + I(t) + R(t) + D(t) &= N \\
	\frac{dS}{dt} + \frac{dE}{dt}  + \frac{dI}{dt} + \frac{dR}{dt} + \frac{dD}{dt} = 0
\end{align}


A model like this expresses the dynamics of an epidemic in greater detail. While the SIR model does distinct between
groups like Susceptible and Infected, it does not differentiate between infected and hospitalized populations. This simplification
makes the modeling process easier, but also ignores real world influences that could influence the simulation outcome.
The SEIRD introduces these groups and aims to simulate a scenario that is much closer to the real world.\newline

\textcolor{red}{move to methods?}\\
This work focused on the modeling capability of SEIRD in the transition stage between susceptibles and exposed. To fulfill this
task, the model was modified to better express phenomena specific to the COVID-19 pandemic. These changes are described in detail
in \hyperref[sec:SEIRDredef]{chapter~\ref*{chap:methodology}, section~\ref*{sec:SEIRDredef}}.


%----------------------------------------------------------------------------------------
%	SECTION 3
%----------------------------------------------------------------------------------------
\section{Variable optimization algorithms}
In order for the presented model equations to work properly, it is important to determine the correct value for each unknown variable.
Different methods to solve differential equations, but in the context of this work the Gauss-Newton and the Particle Swarm Optimization (PSO)
algorithm were used. The following sections will explain the theory behind the two methods.


%-----------------------------------
%	SUBSECTION 1
%-----------------------------------
\subsection{Gauss-Newton algorithm}
\textcolor{red}{add citations, explain least squared better/at all}
\textcolor{red}{describe efficiency and why it is cool}
\label{sec:Gauss}
The Gauss-Newton algorithm is an iterative method, that is used to solve non-linear least square problems. Iterative means, that the algorithm 
starts with an initial solution and tries to improve this solution successively over a number of consecutive steps.
Given a set of $m$ functions, $n$ variables that need to be optimized, a number of target values and a number of starting values, the algorithm tries
optimize the variables in such a way, that the difference between function results and target values is minimized.
The relevant parameters of this algorithms are shown below. 

\begin{align}
	F(\theta) =& (f_0(\theta), f_1(\theta),...,f_m(\theta))^T \\
	\theta^{i} =& (\theta^{i}_0, \theta^{i}_1, \theta^{i}_2,...,\theta^{i}_n)^T \\
	\epsilon^{i} =& F(\theta^{i}) - T \\
	\Delta^{i} =& (\theta^{i}_0 - \theta^{i-1}_0, \theta^{i}_1 - \theta^{i-1}_1, ..., \theta^{i}_m - \theta^{i-1}_m)^T %questionable?
\end{align}

$F(\theta)$ is the vector function, which contains $m$ functions and is
used to estimate the target values. $\theta^i$ is a vector of $n$ estimated input variables in iteration $i$, given to $F(\theta)$.
$\epsilon^i$ expresses the error between the estimation results and the vector of target values $T$ in iteration $i$.
$\Delta^i$ is the vector of adjustment values, that are applied to the estimates of  iteration $i-1$, in order to improve the results of iteration $i$.
For the Gauss-Newton algorithm to be applicable, the condition $m > n$ must be satisfied.\newline

$F(\theta)$ is calculated iterative thought the algorithm and can be expressed in the form of a Taylor Series (eq. \ref*{eq:taylor1}). It can
then be rewritten as the Jacobian matrix $J$.

%note maybe add colors later
\begin{align}
	F(\theta^{i+1}) = F(\theta^{i} + \Delta^{i}) =&
		\begin{pmatrix} f_0(\theta^{i}) &+& \frac{\delta f_0(\theta)}{\delta \theta_0} \Delta^{i}_0 &+& ... &+&  \frac{\delta f_0(\theta)}{\delta \theta_n} \Delta^{i}_n  \\
				. && && && . \\
				. && && && . \\
				f_m(\theta^{i}) &+& \frac{\delta f_0(\theta)}{\delta \theta_0} \Delta^{i}_0 &+& ... &+&  \frac{\delta f_m(\theta)}{\delta \theta_n} \Delta^{i}_n 
		\end{pmatrix} \label{eq:taylor1} \\
	=& \begin{pmatrix} f_0(\theta^{i}) \\ . \\ . \\ f_m(\theta^{i})  \end{pmatrix} +
		\begin{pmatrix} \frac{\delta f_0(\theta)}{\delta \theta_0} & . & . & \frac{\delta f_0(\theta)}{\delta \theta_n}  \\
					   . &&& . \\
					   . &&& . \\
					   \frac{\delta f_m(\theta)}{\delta \theta_0} & . & . & \frac{\delta f_m(\theta)}{\delta \theta_n}
		\end{pmatrix} \begin{pmatrix} \Delta^{i}_0 \\ . \\ . \\ \Delta^{i}_n \end{pmatrix}  \label{eq:taylor2} \\
	=& F(\theta^{i}) + J \Delta^{i} \label{eq:taylor3}
\end{align}

This means the Jacobian Matrix $J$ is defined as such.

\begin{align}
	J_{pq} = \frac{\delta f_p(\theta)}{\delta \theta_q} \quad \text{where } \quad p \in \{0,1,..,m\},\text{ } q \in \{0,1,...,n\}
\end{align}

To minimize the error $\epsilon$ in each following iteration, a connection is established between the current error and the
error of the following iteration, such that the non-linear least squared method can be used. This process is shown below.

\begin{align}
	& \epsilon^{i+1} = F(\theta^{i} + \Delta^{i}) - T, \quad \text{since} \quad F(\theta^{i} + \Delta^{i}) = F(\theta^{i}) + J \Delta^{i} \\
	\Leftrightarrow & \epsilon^{i+1} = F(\theta^{i}) + J \Delta^{i}) - T, \quad \text{since} \quad \epsilon^{i} = F(\theta^{i}) - T \\
	\Leftrightarrow & \epsilon^{i+1} = \epsilon^{i} + J \Delta^{i} \label{eq:taylor4}
\end{align}

\textcolor{red}{put equation in table format?}\\
Since the goal is to minimize the epsilon of the following iteration, equation \ref*{eq:taylor4} can be set equal to zero and reorganized to
take the form of a linear equation $Ax~=~b$. Since $\Delta$ steps usually are sufficiently small, it is assumed that the function is locally
linear. The non-linear least square method is then used to calculate $\Delta$ of the following equation.

\begin{align}
	&\text{min} \| \epsilon^{i} + J \Delta^{i} \| \\
	\Rightarrow & \epsilon^{i} + J \Delta^{i} = 0 \\
	\Leftrightarrow & J \Delta^{i} = -\epsilon^{i} \qquad \text{since $Ax = b$, least squared: } x = (A^T A)^{-1} A^T b \\
	\Leftrightarrow & \Delta^{i} = - (J^T J)^{-1} F^T \epsilon^{i} \label{eq:taylor5}
\end{align}

Therefor equation \ref*{eq:taylor5} can be used to calculate a suitible $\Delta$ in order to decrease the error in each iteration. \newline

The core steps of the Gauss-Newton method are listed below as pseudo-code.

%\begin{enumerate}
%	\item Start with an initial estimate $\theta^{(0)}$ for the optimal variables
%	\item Calculate the error $\epsilon$ between the target data and the function results with the estimated values. If $\epsilon$
%		is not zero or fulfills a breaking condition, continue. Otherwise return the current estimate 
%	\item Calculate the Jacobian-Matrix $J$ using the functions and the current estimate
%	\item Use $J$ and $\epsilon$ to calculate the difference $\Delta$ by which the estimate of the current iteration is
%		modified. Update the current estimate and continue from step 2
%\end{enumerate}


\begin{algorithm}
	\caption{Gauss-Newton algorithm pseudocode}
	\begin{algorithmic}
		\State Set $T$ = target values;
		\State Set $\theta^{0}$ = initial estimate for optimal varialbes;
		\State Calculate initial error $\epsilon^0$ = $F(\theta^0) - T$;
		\State Set i=0;
		\While {\{$\epsilon^i$ != 0 \B{or} (breaking conditions \B{not} fulfilled)\}}
			\State Calculate the Jacobian-Matrix $J$ using $\theta^i$;
			\State Use $J$ and $\epsilon^i$ to calculate estimate adjustment $\Delta^i$;
			\State Set $\theta^{i}$ = $\theta^{i} + \Delta^i$;
			\State Set i = i+1;
			\State Calculate $\epsilon^i$
		\EndWhile;
		\State \B{return} $\theta^i$;
	\end{algorithmic}
\end{algorithm}


%-----------------------------------
%	SUBSECTION 2
%-----------------------------------

\subsection{Particle Swarm Optimization}
\label{sec:PSO}
Like the Gauss-Newton algorithm, $Particle Swarm Optimization$ (PSO) is a method used to iteratively optimize a problem. The general
concept of PSO is to generate a number of candidate solutions (particles) per iteration, quantify the quality of each solution, take the best
candidate of the sum of all particles (swarm) as the current best solution and try to improve this solution in following iterations.
Improvements are made by (randomly) altering the candidate solutions in hopes of finding a new solution, that improves on the previous best.
If suitable breaking conditions are met or the maximum number of iterations has passed, the algorithm stops and returns the current best solution.
\newline

In order to find the best solution in the defined search space as quickly and efficiently as possible, additional conditions are applied
to each particle. Particles are grouped together and a current best group solution is recorded, in addition to the current global optimal solution.
Modifications of the solution of each particle per iteration are random, but in some way biased towards both the group and global optimum.
This is meant to direct a greater number of particles to the area of the most optimal solutions, that are currently known.
However, these solution may not be the global optimum for the defined search space and there is no guarantee, that improvements will be made in any
given iteration. As a trade off, the problem function does not need to be differentiable like it is the case for the Gauss-Newton algorithm. \newline

There are many types of PSO algorithms. The following is a general setup:\newline
A problem space is defined, as well as a cost function $L$, which takes a number of $n$ input values in the from of a vector $\theta$ and projects them
to a real number $\epsilon$. The goal is to minimize this function. Each particle $x_i$ is defined by a number of variables. These variables are
its candidate solution (position) $p_i$, the positional change of the particle (velocity) $v_i$ and its group index $i_i$. Each group is defined by
three different variables. Its group index $i_j$ and the current best group solution $g_j$. Finally there is a global variable, the global best
(swarm) solution $S$.

\begin{align}
	x_i =& (p_i, v_i, i_i)
	g_i =& (v_i, g_i)
\end{align}

Shown below is a PSO algorithm, pseudocode example.

\begin{algorithm}
	\caption{Particle Swarm Optimization pseudocode}
	\begin{algorithmic}
		\State Set $T, S$ = target values, uniformly chosen random solution;
		\State Set $g, p$ = number of groups, number of particles;
		%\State Set $s$ = random solution in the defined search space;
		%\State Set G = number of goups;
		\For {each group $x_j$, where $j \in G = \{0,1,...,g\}$}
			\State Set $i_j$ = group index;
			\State Set $g_j$ = random solution in defined search space;
			\If {$f(S) > f(g_j)$}
				\State Set $S = g_j$;
			\EndIf
		\EndFor
		%\State Set P = number of paricles;
		\For {each paricle $x_i$, where $i \in P = \{0,1,...,p\}$}
			\State Set $p_i$ = uniformly chosen random starting position;
			\State Set $v_i$ = no initial change;
			\State Set $i_i$ = index of a group;
			\If {$f(S) > f(p_i)$}
				\State Set $S = p_i$;
				\State Set $g_i.g = p_i$;
			\ElsIf {$f(g_i.g) > f(p_i)$}
				\State Set $g_i.g = p_i$;
			\EndIf
		\EndFor
		\While {$f(s) != 0$ \B{or} (breaking conditions are \B{not} met)}
			\For {each particle $x_i$}
				\State Set velocity $v_i$ to random new value, biased by $g_i.g$ and $s$;
				\State Update particle position $p_i$ based on velocity $v_i$;
				\If {$f(S) > f(p_i)$}
					\State Set $S = p_i$;
					\State Set $g_i.g = p_i$;
				\ElsIf {$f(g_i.g) > f(p_i)$}
					\State Set $g_i.g = p_i$;
				\EndIf
			\EndFor
		\EndWhile
		\State \B{return} $S$;
	\end{algorithmic}
\end{algorithm}

\textcolor{red}{make pseudoalgo look nicer and easier to understand}


