% Chapter Template

\chapter{Discussion} % Main chapter title
\label{chap:discussion} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}
\textcolor{red}{weave in story, why only susceptibles were examined in experiments!}
\textcolor{red}{add images to discussion?}


This chapter will summarize, discuss and interpret the results of the \hyperref[chap:results]{previous section
\ref*{chap:results}}. We will start by discussing and evaluating the accuracy of the model regarding the reproduction
of real world data, as the ability to predict future outcomes with fewer data points. After that, we will use the
analyze the results of the loss function analysis and the sensitivity analysis in order to better understand the results
and identify potential issues and areas of improvement. Lastly an outlook will be given for possible strategies and
ideas to improve both the implementation of the model and the model itself.
%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

\section{Simulation results and sensitivity analysis}
The following section will discuss the results of the simulations, the optimization process as well as the sensitivity
analysis performed in this context.
\textcolor{red}{add more text?}


%-----------------------------------
%	SUBSECTION 2
%-----------------------------------
\subsection{Simulation accuracy and optimization issues}
\hyperref[sec:sim_res]{Section \ref*{sec:sim_res}} has presented the simulation results produced by the current implementation
of the SEIRD model. It can be seen, that the accuracy of the simulation somewhat depends on the amount of data points supplied.
While the median susceptible deviation of all regions ranged between -80 and +140 percent, most the regions showed reasonable
amounts of deviation from the original data. In case of the 76 data point simulation, 21 of 26 regions showed an
absolute median deviation of 50 percent or less. Similar behavior was observed for the 60 and 50 data point simulations, in which
21 and 22 regions respectively showed an absolute median deviation of 50 percent or less. While the simulation accuracy of single regions
might differ between experiments (\textcolor{red}{double check this!}), results are overall comparable.\newline

Predicting the original data proved more difficult. While the calculated trend for the 60 data point experiment was comparable
to the simulation with 76 data points, the 50 data point simulation showed strong differences and proved to be mostly inaccurate.\\
\textcolor{red}{needs hard data to support these claims, check python code on how to extract data points and add to results.
Then come back and redo this}\\ %note

A consistent observation throughout all experiments was a strong trend of the simulated regions to negatively deviate from the
original data. This means that the simulations generally predicted lower amounts of exposed individuals than seen in the original
data. In all three experiments 19 of the 26 regions deviated in this way from the original data. This may seem odd at first,
since it could be expected, that the optimal state of the model is reached, when a maximum amount of regions are close to matching
their original data. However, if  closer attention if given to the way optimization was done in these experiments this phenomenon
becomes much clearer. All adjustments were done based on minimizing the loss function. The loss function itself was defined in
\hyperref[eq:loss_newton]{equation \ref*{eq:loss_newton}} as the sum of the square difference between the simulated and the original
data of all data points and each region. This means that a regions individual influence on the adjustment of model variables
increases as the total difference between its simulated and original data grows. This effect is further increased, since the
difference between original and simulated data is squared before it is summed up. Thus leading to a quadratic increase in influence
of highly deviating regions relative to the other regions. \newline

Combining this knowledge with the observations of the model results, helped us to identify three factors that can be used to explain
the optimization behavior of the current implementation of the model. \textcolor{red}{rephrase}.

\begin{enumerate}[label=\arabic*.]
	\item The percentage deviation of the simulated data from the original data.
	\item The time at which the deviation occurs.
	\item The total population of each individual region.
\end{enumerate}

The first factor is relatively obvious. The most influential region in our 76 data point experiment was ``Offenbach'' with
a mean deviation of -54 percent after optimization. This means, that about half of the simulated data points of this region
had less than half as many exposed individuals as the original data. Naturally this should cause adjustments of the model variables,
but it also transitions well to the second factor. The point in time when the deviation occurs has great influence on the 
overall influence of the region on the loss. In our experiments, many regions displayed strong deviations during the first
days of the simulation going up to 467.37 percent in the case of ``Limburg-Weilburg''. However, these differences were observed
during the first days of the simulation, where infection events in the real world were more sporadic, due to smaller numbers of
already infected individuals. This also means that the total difference between simulated and original data is usually much
smaller during the early days of the simulation, since the expected number of infection events at that point in time is much smaller.
In the case mentioned before, 467.37 percent deviation translated to a number of 4.6737 additional infection events in the simulation,
compared to only one infected person in the original data set. The influence on the overall loss was extremely low in this case.
\textcolor{red}{add a bit more information here like total loss for better comparability} %note
The third major factor is the overall population of each respective region, since regions with a high population will tend to
produce higher differences between simulated and original data compared to regions with smaller populations. This remains true, even
if the percentage deviation between simulated and original data is relatively small compared to other regions. A good example for this
is the region ``Frankfurt-am-Main'', which was the third strongest loss contributor in the optimized version of the model, despite
only showing a median deviation of about 21 percent between simulated and original data. This is caused by the high population of
this region, which makes up about 12.14 percent, of the total population of Hesse.\newline

All these effects combined mean, that both outliers and high population areas can and likely will disproportionately effect the
optimization process of the model. The question is, whether this effect if desired or not. Arguably the model optimization was
working in terms of minimizing the optimal loss. A smaller overall loss was achieved by reducing the accuracy of many smaller
regions in favor of outlier regions and regions with higher populations. We will discuss alternatives to this method in the
following sections.

\textcolor{red}{add more info about Frankfurt and population; note that loss during optimization process was not reproduced.
Optimal variable state can only give hints to internal workings}

%-----------------------------------
%	SUBSECTION 2
%-----------------------------------
\subsection{Variable influence on the susceptible population}
\textcolor{red}{what else to discuss?}\newline %note
In order to quantify (\textcolor{red}{rephrase, not quantified}) the influence of model variables on the simulation of the
susceptible population, 4000 simulations were performed with randomly chosen $\alpha$ and $q$ values bound between 0.05 and 0.35
and 5.5 and 8.0 respectively. The reason, why only those two variables were observed can be explained by looking at equations
\ref{eq:SEIRD1_S} and \ref{eq:SEIRD1_E} from chapter \ref*{chap:theory},
section \ref*{sec:SEIRD}. The only group observed in these experiments was the susceptible group, hence equation \ref*{eq:SEIRD1_S}
can be used to deduce all relevant variables that influence this group. Influencing variables are the number of susceptibles \S,
the number of exposed \B{E} and the rate at which individuals transition from \B{S} to \B{E}. While group \B{S} only changes based on this
equation, \B{E} also changes based on equation \ref*{eq:SERD1_E}. Looking at this equation we can see, that the changes of \B{E} depend
again on \B{S}, \B{E} and $\alpha$, but also on $q$. Other variables or groups do not influence the change of \B{S}, and are therefor not
relevant for this simulation. \newline

The simulations were then used to create a topographic map of the influence of the variables on the loss of the model. The results
showed, that both $\alpha$ and $q$ influence the loss. \hyperref[fig:sensitivity_zoom0]{Figure \ref*{fig:sensitivity_zoom0}} shows,
that the overall loss of the simulation remains fairly identical, until $\alpha$ reaches a value off about 2.3. Before this,
changes in $q$ seem to have very little impact on the loss of the simulation. This makes sense, when looking at the relation of
\ref*{eq:SEIRD1_S} and \ref*{eq:SEIRD1_E}. The variable $\alpha$ controls the "outflow" of population from \B{S} to \B{E}, while $q$
controls the "outflow" of population from \B{E} to either \B{I} or \B{D}. However, an outflow from \B{E} is only relevant, when there are
individuals present in this group in the first place. Therefor, an $\alpha$ value below a certain threshold seems to either permit
no transition from \B{S} to \B{E} or keep the population of \B{E} so small, that the influence of the outflow $q$ are minimal.\newline

Once this threshold is overcome though, $q$ seems to have a more substantial impact. While the simulations show
an increase in loss regardless of $q$, the increase is much stronger with a higher $q$, than with a smaller one.
As a reminder, in equation~\ref*{eq:SEIRD1_E} the outflow of individuals from the exposed group is defined as $\text{\B{E}}/q$.
This means, that a higher numeric value for $q$ means a lower rate of outflow of the exposed class. Inversely a lower $q$ results
in a higher outflow rate. Therefor the loss is higher at a lower outflow rate out of \B{E} and smaller at a higher outflow rate.
This effect is likely caused by an $\alpha$ value that starts out to small and thereby fails to transition enough susceptibles
to the group of exposed. The steady loss observed throughout most of the map likely means that the number of simulated susceptibles
remains mostly stable, while the number of transitions from \B{S} to \B{E} in the original data steadily increases. However, since 
there is a maximum difference between the initial number of susceptibles (at which the simulated likely remains
\textcolor{red}{maybe check this for truth}) and the original data, the total loss cannot supersede a set maximum loss.
But, once $\alpha$ exceeds the threshold of about 2.3, the situation begins to change and the amount of transition events from
\B{S} to \B{E} begins to increase. If the number of members in the exposed group is not decreased by transition events to either
\B{I} or \B{D}, which is governed by the variable $q$, a feedback loop is the result. In this case, more transition to \B{E} at a time
step will cause an increased transition to \B{E} in the following time step and so on. This process will continue, as long as there is
either no or very little outflow out of \B{E}. Since the difference between simulated and original data can be much larger in this case,
an strong increase in loss is the result. $q$ therefor serves as a counterbalance the effects of $\alpha$. All of these results show
that the model itself is working as intended.
\textcolor{red}{talk more about the other images in this section and explain that results are in optimum}

 It is also interesting, that the there appears to be a set of possible values where $\alpha$ and $q$ are in an equilibrium and 
 minimize the overall loss of the simulation. The images in \hyperref[fig:sensitivity_zoom1]{Figure \ref*{fig:sensitivity_zoom1}}
 show this very clearly. Between the $\alpha$ and $q$ values of 1.7 and 8.0 and 2.3 and 5.5 exists an optimal region, where the
 loss appears to be either minimal or at least close to minimal. The optimal region has a diagonal shape and suggests a linear
 relationship between $\alpha$ and $q$, where loss remains close to optimal, if an increase in $\alpha$ (increase in infection events)
 is compensated by a decrease in $q$ (a decrease in infection events) and vice versa. The optimal values $\alpha = 0.198$ and
 $q = 6.675$ is also within this optimal region. This further supports the idea that the model and the optimization is working as
 intended.

%----------------------------------------------------------------------------------------
%	SECTION 2
%----------------------------------------------------------------------------------------

\section{Model adjustments and solutions}
In the previous section we discussed the results, different effect observed in the experiments and pointed out potential areas of
improvement. This section will focus on discussing possible adjustments to the model in order to increase the overall performance.


%-----------------------------------
%	SUBSECTION 1
%-----------------------------------
\subsection{General approach of optimizing different regions}
- weight of different regions
	- normalization of difference (maybe percentage deviation in loss)
	- optimize for maximum amount of regions optimal
- Diffusion
- density/population dependent variables

\subsection{}

%----------------------------------------------------------------------------------------
%	SECTION 3
%----------------------------------------------------------------------------------------

\section{Outlook}



